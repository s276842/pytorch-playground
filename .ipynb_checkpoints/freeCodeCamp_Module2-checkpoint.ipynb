{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training dataset\n",
    "dataset = MNIST(root='data/', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', train=False)\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch doesn't know how to work with images. We need to convert the images into tensors. We can do this by specifying a transform while creating our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# MNIST dataset (images and labels)\n",
    "dataset = MNIST(root='data/', \n",
    "                train=True,\n",
    "                transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and validation sets\n",
    "Division in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x000001EF4F70D208>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's essential to choose a random sample for creating a validation set. Training data is often sorted by the target labels, i.e., images of 0s, followed by 1s, followed by 2s, etc. If we create a validation set using the last 20% of images, it would only consist of 8s and 9s. In contrast, the training set would contain no 8s or 9s. Such a training-validation would make it impossible to train a useful model.\n",
    "\n",
    "We can now create data loaders to help us load the data in batches. We'll use a batch size of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set shuffle=True for the training data loader to ensure that the batches generated in each epoch are different. This randomization helps generalize & speed up the training process. On the other hand, since the validation data loader is used only for evaluating the model, there is no need to shuffle the images.\n",
    "\n",
    "###### Logistic regression\n",
    "The model is linear so it expects each training example to be a vector, each 1x28x28 image tensor is flattened into a vector of size 784 (28*28) before being passed into the model.\n",
    "\n",
    "The output for each image is a vector of size 10, with each element signifying the probability of a particular target label (i.e., 0 to 9). The predicted label for an image is simply the one with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MnistModel(\n",
       "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "# Logistic regression model\n",
    "class MnistModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_size = 28*28\n",
    "        self.num_classes = 10\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(self.input_size, self.num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, self.input_size)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "model = MnistModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of MnistModel(\n",
       "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1, 28, 28])\n",
      "outputs.shape :  torch.Size([128, 10])\n",
      "Sample outputs for the first two images :\n",
      " tensor([[-0.0337,  0.0728, -0.2402,  0.0713,  0.0534, -0.2000, -0.1990,  0.1268,\n",
      "         -0.0262, -0.0581],\n",
      "        [-0.0588, -0.0938, -0.0157,  0.0035,  0.0274,  0.2004, -0.1337,  0.1261,\n",
      "          0.0623, -0.0990]])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape)\n",
    "    outputs = model(images)\n",
    "    break\n",
    "\n",
    "print('outputs.shape : ', outputs.shape)\n",
    "print('Sample outputs for the first two images :\\n', outputs[:2].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to turn the output to probabilities. We can do this first exponentiating and then normalizing\n",
    "$$S(y_i) = \\frac{e^{y_i}}{\\sum_je^{y_j}}$$\n",
    "\n",
    "We apply softmax to *dim=1*. Note that dim 0 are the batches 128, while dim 1 are the 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10])\n",
      "Sample probabilities:\n",
      " tensor([[0.1002, 0.1115, 0.0815, 0.1113, 0.1093, 0.0849, 0.0849, 0.1177, 0.1010,\n",
      "         0.0978],\n",
      "        [0.0936, 0.0904, 0.0978, 0.0997, 0.1021, 0.1213, 0.0869, 0.1126, 0.1057,\n",
      "         0.0899]])\n",
      "Sum:  0.9999998807907104\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# Apply softmax for each output row\n",
    "print(outputs.shape)\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "# Look at sample probabilities\n",
    "print(\"Sample probabilities:\\n\", probs[:2].data)\n",
    "\n",
    "# Add up the probabilities of an output row\n",
    "print(\"Sum: \", torch.sum(probs[0]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can determine the predicted label for each image by simply choosing the index of the element with the highest probability in each output row. We can do this using torch.max, which returns each row's largest element and the corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: \n",
      "tensor([7, 5, 1, 5, 7, 4, 7, 4, 1, 3, 4, 5, 2, 7, 9, 5, 2, 8, 3, 5, 8, 3, 4, 4,\n",
      "        1, 7, 4, 3, 4, 4, 8, 7, 2, 3, 2, 1, 5, 9, 5, 4, 7, 1, 4, 7, 0, 7, 1, 7,\n",
      "        7, 4, 3, 3, 3, 5, 7, 1, 5, 4, 2, 0, 3, 1, 3, 1, 8, 7, 8, 2, 5, 4, 7, 6,\n",
      "        8, 3, 6, 0, 4, 5, 3, 3, 1, 2, 1, 8, 7, 7, 2, 8, 1, 4, 3, 7, 4, 4, 4, 4,\n",
      "        7, 5, 3, 7, 2, 5, 4, 3, 3, 7, 7, 2, 8, 1, 1, 4, 2, 5, 2, 7, 9, 5, 4, 9,\n",
      "        2, 7, 2, 9, 8, 7, 7, 2])\n",
      "probability: \n",
      "tensor([0.1177, 0.1213, 0.1246, 0.1126, 0.1150, 0.1500, 0.1411, 0.1151, 0.1247,\n",
      "        0.1310, 0.1225, 0.1435, 0.1154, 0.1212, 0.1167, 0.1303, 0.1212, 0.1289,\n",
      "        0.1276, 0.1198, 0.1316, 0.1235, 0.1251, 0.1309, 0.1340, 0.1423, 0.1260,\n",
      "        0.1207, 0.1239, 0.1431, 0.1189, 0.1143, 0.1224, 0.1244, 0.1346, 0.1217,\n",
      "        0.1114, 0.1122, 0.1155, 0.1334, 0.1362, 0.1348, 0.1131, 0.1401, 0.1172,\n",
      "        0.1370, 0.1265, 0.1237, 0.1283, 0.1134, 0.1322, 0.1154, 0.1215, 0.1398,\n",
      "        0.1286, 0.1211, 0.1197, 0.1348, 0.1285, 0.1208, 0.1287, 0.1225, 0.1283,\n",
      "        0.1213, 0.1302, 0.1195, 0.1222, 0.1438, 0.1444, 0.1129, 0.1309, 0.1140,\n",
      "        0.1175, 0.1247, 0.1142, 0.1182, 0.1196, 0.1463, 0.1144, 0.1298, 0.1231,\n",
      "        0.1177, 0.1191, 0.1230, 0.1166, 0.1257, 0.1155, 0.1190, 0.1327, 0.1174,\n",
      "        0.1327, 0.1455, 0.1259, 0.1229, 0.1105, 0.1149, 0.1445, 0.1222, 0.1164,\n",
      "        0.1539, 0.1151, 0.1223, 0.1265, 0.1137, 0.1338, 0.1442, 0.1208, 0.1177,\n",
      "        0.1200, 0.1243, 0.1295, 0.1176, 0.1343, 0.1248, 0.1213, 0.1331, 0.1083,\n",
      "        0.1359, 0.1217, 0.1385, 0.1287, 0.1375, 0.1186, 0.1120, 0.1214, 0.1640,\n",
      "        0.1219, 0.1138], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(f\"predictions: \\n{preds}\")\n",
    "print(f\"probability: \\n{max_probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define an accuracy function counting the number of predictions == actual labels.\n",
    "\n",
    "The == operator performs an element-wise comparison of two tensors with the same shape and returns a tensor of the same shape, containing True for unequal elements and False for equal elements. Passing the result to torch.sum returns the number of labels that were predicted correctly. Finally, we divide by the total number of images to get the accuracy.\n",
    "\n",
    "Note that we don't need to apply softmax to the outputs since its results have the same relative order. This is because e^x is an increasing function, i.e., if y1 > y2, then e^y1 > e^y2. The same holds after averaging out the values to get the softmax.\n",
    "\n",
    "Let's calculate the accuracy of the current model on the first batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1484)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(outputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is an excellent way for us (humans) to evaluate the model. However, we can't use it as a loss function for optimizing our model using gradient descent for the following reasons:\n",
    "\n",
    "It's not a differentiable function. torch.max and == are both non-continuous and non-differentiable operations, so we can't use the accuracy for computing gradients w.r.t the weights and biases.\n",
    "\n",
    "It doesn't take into account the actual probabilities predicted by the model, so it can't provide sufficient feedback for incremental improvements.\n",
    "\n",
    "For these reasons, accuracy is often used as an evaluation metric for classification, but not as a loss function. A commonly used loss function for classification problems is the cross-entropy, which has the following formula:\n",
    "\n",
    "<img src=\"https://i.imgur.com/VDRDl1D.png\">\n",
    "\n",
    "Note that only the value $i$ s.t. $y_i=1$ is kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2924, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "\n",
    "# Loss for current batch of data\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that cross-entropy is the negative logarithm of the predicted probability of the correct label averaged over all training samples. Therefore, one way to interpret the resulting number e.g. 2.23 is look at e^-2.23 which is around 0.1 as the predicted probability of the correct label, on average. The lower the loss, The better the model. \n",
    "\n",
    "##### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    history = [] # for recording epoch-wise results\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.7358, val_acc: 0.8433\n",
      "Epoch [1], val_loss: 0.7181, val_acc: 0.8457\n",
      "Epoch [2], val_loss: 0.7021, val_acc: 0.8481\n",
      "Epoch [3], val_loss: 0.6876, val_acc: 0.8497\n",
      "Epoch [4], val_loss: 0.6742, val_acc: 0.8514\n",
      "Epoch [5], val_loss: 0.6619, val_acc: 0.8524\n",
      "Epoch [6], val_loss: 0.6506, val_acc: 0.8538\n",
      "Epoch [7], val_loss: 0.6401, val_acc: 0.8549\n",
      "Epoch [8], val_loss: 0.6303, val_acc: 0.8569\n",
      "Epoch [9], val_loss: 0.6212, val_acc: 0.8573\n",
      "Epoch [10], val_loss: 0.6127, val_acc: 0.8584\n",
      "Epoch [11], val_loss: 0.6047, val_acc: 0.8601\n",
      "Epoch [12], val_loss: 0.5972, val_acc: 0.8616\n",
      "Epoch [13], val_loss: 0.5901, val_acc: 0.8623\n",
      "Epoch [14], val_loss: 0.5835, val_acc: 0.8631\n",
      "Epoch [15], val_loss: 0.5771, val_acc: 0.8637\n",
      "Epoch [16], val_loss: 0.5712, val_acc: 0.8645\n",
      "Epoch [17], val_loss: 0.5655, val_acc: 0.8656\n",
      "Epoch [18], val_loss: 0.5601, val_acc: 0.8662\n",
      "Epoch [19], val_loss: 0.5550, val_acc: 0.8668\n",
      "Epoch [20], val_loss: 0.5501, val_acc: 0.8671\n",
      "Epoch [21], val_loss: 0.5455, val_acc: 0.8673\n",
      "Epoch [22], val_loss: 0.5410, val_acc: 0.8683\n",
      "Epoch [23], val_loss: 0.5368, val_acc: 0.8688\n",
      "Epoch [24], val_loss: 0.5327, val_acc: 0.8695\n",
      "Epoch [25], val_loss: 0.5287, val_acc: 0.8699\n",
      "Epoch [26], val_loss: 0.5250, val_acc: 0.8704\n",
      "Epoch [27], val_loss: 0.5214, val_acc: 0.8705\n",
      "Epoch [28], val_loss: 0.5179, val_acc: 0.8709\n",
      "Epoch [29], val_loss: 0.5146, val_acc: 0.8711\n",
      "Epoch [30], val_loss: 0.5113, val_acc: 0.8721\n",
      "Epoch [31], val_loss: 0.5082, val_acc: 0.8726\n",
      "Epoch [32], val_loss: 0.5052, val_acc: 0.8732\n",
      "Epoch [33], val_loss: 0.5023, val_acc: 0.8740\n",
      "Epoch [34], val_loss: 0.4996, val_acc: 0.8744\n",
      "Epoch [35], val_loss: 0.4968, val_acc: 0.8752\n",
      "Epoch [36], val_loss: 0.4942, val_acc: 0.8754\n",
      "Epoch [37], val_loss: 0.4917, val_acc: 0.8758\n",
      "Epoch [38], val_loss: 0.4892, val_acc: 0.8764\n",
      "Epoch [39], val_loss: 0.4869, val_acc: 0.8763\n",
      "Epoch [40], val_loss: 0.4845, val_acc: 0.8770\n",
      "Epoch [41], val_loss: 0.4823, val_acc: 0.8773\n",
      "Epoch [42], val_loss: 0.4801, val_acc: 0.8776\n",
      "Epoch [43], val_loss: 0.4780, val_acc: 0.8782\n",
      "Epoch [44], val_loss: 0.4759, val_acc: 0.8783\n",
      "Epoch [45], val_loss: 0.4739, val_acc: 0.8788\n",
      "Epoch [46], val_loss: 0.4720, val_acc: 0.8789\n",
      "Epoch [47], val_loss: 0.4701, val_acc: 0.8793\n",
      "Epoch [48], val_loss: 0.4682, val_acc: 0.8797\n",
      "Epoch [49], val_loss: 0.4665, val_acc: 0.8798\n"
     ]
    }
   ],
   "source": [
    "history1 = fit(50, 0.001, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.4357922673225403, 'val_acc': 0.8885482549667358}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root='data/', train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size)\n",
    "\n",
    "evaluate(model, test_loader)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6 , Predicted: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "img, label = test_dataset[np.random.randint(len(test_dataset))]\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2 , Predicted: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wrong\n",
    "while True:\n",
    "    img, label = test_dataset[np.random.randint(len(test_dataset))]\n",
    "    if label != predict_image(img, model):\n",
    "        break\n",
    "plt.imshow(img[0], cmap='gray')\n",
    "print('Label:', label, ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New instatiated model {'val_loss': 2.3092503547668457, 'val_acc': 0.06416015326976776}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': 0.43419450521469116, 'val_acc': 0.8875976800918579}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save state of the model\n",
    "torch.save(model.state_dict(), 'mnist-logistic.pth')\n",
    "\n",
    "# Instantiate new object\n",
    "model2 = MnistModel()\n",
    "result = evaluate(model2, test_loader)\n",
    "print(f\"New instatiated model {result}\")\n",
    "\n",
    "# Load model\n",
    "model2.load_state_dict(torch.load('mnist-logistic.pth'))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "result = evaluate(model2, test_loader)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
