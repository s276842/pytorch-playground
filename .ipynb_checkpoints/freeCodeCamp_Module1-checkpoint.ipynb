{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number = torch.tensor(4.)\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.tensor([1., 2, 3, 4])\n",
    "print(t2)\n",
    "t2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient\n",
    "\n",
    "We can combine tensors with the usual arithmetic operations. Using *requires_grad* in the initializaiton of a pytorch variablwe signal to pytorchspecify that it is a variable and so we can compute its partial derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensors.\n",
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)\n",
    "x, w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = w*x + b, \\qquad x = 3, w = 4, b = 5 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arithmetic operations\n",
    "y = w * x + b\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute derivates, we can invoke the *.backward* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derivatives\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: None\n",
      "dy/dw: tensor(3.)\n",
      "dy/db: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Display gradients\n",
    "print('dy/dx:', x.grad)\n",
    "print('dy/dw:', w.grad)\n",
    "print('dy/db:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear regression\n",
    "<img src=\"https://i.imgur.com/6Ujttb4.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start we random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3134, -0.0397, -0.6176],\n",
      "        [ 1.7633,  0.5467,  1.9362]], requires_grad=True)\n",
      "tensor([-0.0114,  0.5444], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is simply a function that performs a matrix multiplication of the inputs and the weights w (transposed) and adds the bias b (replicated for each observation).\n",
    "\n",
    "<img src=\"https://i.imgur.com/WGXLFvA.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, w, b):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-125.1015,  249.1490],\n",
      "        [-162.5446,  333.0285],\n",
      "        [-155.4108,  339.5048],\n",
      "        [-158.5313,  275.5482],\n",
      "        [-137.6736,  310.2259]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs, w, b)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 181.1015, -179.1490],\n",
      "        [ 243.5446, -232.0285],\n",
      "        [ 274.4108, -206.5048],\n",
      "        [ 180.5313, -238.5482],\n",
      "        [ 240.6736, -191.2259]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compare with targets\n",
    "print(targets-preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate how good/bad the model is doing we introduce a *loss* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss - average of swuared residuals\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(47997.6797, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In average each prediction differs from the actual value *loss*. Our goal is to minimize this quadratic function with gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3134, -0.0397, -0.6176],\n",
      "        [ 1.7633,  0.5467,  1.9362]], requires_grad=True)\n",
      "tensor([[-18855.4746, -20240.8555, -12563.3711],\n",
      "        [ 17936.9805,  17741.6816,  11348.5225]])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for weights\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decrease the loss we must make a small step in the direction opposite of the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * lr\n",
    "    b -= b.grad * lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use torch.no_grad to indicate to PyTorch that we shouldn't track, calculate, or modify gradients while updating the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(32624.2441, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's verify that the loss is actually lower\n",
    "loss = mse(model(inputs, w, b), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, we reset the gradients to zero by invoking the .zero_() method. We need to do this because PyTorch accumulates gradients. Otherwise, the next time we invoke .backward on the loss, the new gradient values are added to the existing gradients, which may lead to unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 epochs\n",
    "def train_model(w, b, k):\n",
    "    loss_list = []\n",
    "    lr = 1e-5\n",
    "    for i in range(100):\n",
    "        preds = model(inputs, w, b)\n",
    "        loss = mse(preds, targets)\n",
    "        loss_list.append(loss)\n",
    "#         print(f\"epoch[{i}]: loss1 = {loss:.2f}\")\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w -= w.grad * lr\n",
    "            b -= b.grad * lr\n",
    "            if( i%k == 0):\n",
    "                w.grad.zero_()\n",
    "                b.grad.zero_()\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses1 = train_model(w, b,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = torch.randn(2, 3, requires_grad=True)\n",
    "b2 = torch.randn(2, requires_grad=True)\n",
    "losses2 = train_model(w2, b2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(100), losses1, label=\"1\")\n",
    "plt.plot(range(100), losses2, label=\"2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression using PyTorch built-ins\n",
    "We've implemented linear regression & gradient descent model using some basic tensor operations. However, since this is a common pattern in deep learning, PyTorch provides several built-in functions and classes to make it easy to create and train models with just a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.],\n",
       "        [ 74.,  66.,  43.],\n",
       "        [ 91.,  87.,  65.],\n",
       "        [ 88., 134.,  59.],\n",
       "        [101.,  44.,  37.],\n",
       "        [ 68.,  96.,  71.],\n",
       "        [ 73.,  66.,  44.],\n",
       "        [ 92.,  87.,  64.],\n",
       "        [ 87., 135.,  57.],\n",
       "        [103.,  43.,  36.],\n",
       "        [ 68.,  97.,  70.]])"
      ]
     },
     "execution_count": 917,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 919,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorDataset allows us to access a small section of the training data using the array indexing notation ([0:3] in the above code). It returns a tuple with two elements. The first element contains the input variables for the selected rows, and the second contains the targets.\n",
    "\n",
    "We'll also create a DataLoader, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Define data loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[103.,  43.,  36.],\n",
      "        [ 68.,  97.,  70.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 88., 134.,  59.],\n",
      "        [101.,  44.,  37.]])\n",
      "tensor([[ 20.,  38.],\n",
      "        [102., 120.],\n",
      "        [ 22.,  37.],\n",
      "        [118., 132.],\n",
      "        [ 21.,  38.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model loading\n",
    "Instead of initializing the weights & biases manually, we can define the model using the nn.Linear class from PyTorch, which does it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3117,  0.1121,  0.0023],\n",
      "        [-0.1589, -0.5105,  0.1858]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3957, 0.3833], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models also have a helpful .parameters method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3117,  0.1121,  0.0023],\n",
       "         [-0.1589, -0.5105,  0.1858]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.3957, 0.3833], requires_grad=True)]"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-14.7476, -37.4269],\n",
       "        [-17.9550, -47.1046],\n",
       "        [-11.5644, -71.0671],\n",
       "        [-26.4928, -30.8973],\n",
       "        [-10.1859, -46.5784],\n",
       "        [-15.1715, -37.0753],\n",
       "        [-18.0648, -46.4083],\n",
       "        [-11.8738, -71.0401],\n",
       "        [-26.0690, -31.2489],\n",
       "        [ -9.8718, -46.2337],\n",
       "        [-14.8574, -36.7306],\n",
       "        [-18.3788, -46.7530],\n",
       "        [-11.4546, -71.7634],\n",
       "        [-26.8069, -31.2420],\n",
       "        [ -9.7620, -46.9300]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15389.6973, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Import nn.functional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that model.parameters() is passed as an argument to optim.SGD so that the optimizer knows which matrices should be modified during the update step. In complex models different branches of the structure can use different optimizers, so not all the parameters of the network must be passed.\n",
    "Also, we can specify a learning rate that controls the amount by which the parameters are modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 358.9918\n",
      "Epoch [20/100], Loss: 512.6168\n",
      "Epoch [30/100], Loss: 55.0282\n",
      "Epoch [40/100], Loss: 100.0373\n",
      "Epoch [50/100], Loss: 119.1583\n",
      "Epoch [60/100], Loss: 5.1483\n",
      "Epoch [70/100], Loss: 25.4652\n",
      "Epoch [80/100], Loss: 40.4350\n",
      "Epoch [90/100], Loss: 19.7466\n",
      "Epoch [100/100], Loss: 15.7992\n"
     ]
    }
   ],
   "source": [
    "fit(100, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
