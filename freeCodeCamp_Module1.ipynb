{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number = torch.tensor(4.)\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = torch.tensor([1., 2, 3, 4])\n",
    "print(t2)\n",
    "t2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient\n",
    "\n",
    "We can combine tensors with the usual arithmetic operations. Using *requires_grad* in the initializaiton of a pytorch variabl, we signal to trace its gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensors.\n",
    "x = torch.tensor(3.)\n",
    "w = torch.tensor(4., requires_grad=True)\n",
    "b = torch.tensor(5., requires_grad=True)\n",
    "x, w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = w*x + b, \\qquad x = 3, w = 4, b = 5 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Arithmetic operations\n",
    "y = w * x + b\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute derivates, we can invoke the *.backward* method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute derivatives\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: None\n",
      "dy/dw: tensor(3.)\n",
      "dy/db: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Display gradients\n",
    "print('dy/dx:', x.grad)\n",
    "print('dy/dw:', w.grad)\n",
    "print('dy/db:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42, 42],\n",
       "        [42, 42],\n",
       "        [42, 42]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "t2 = torch.full((3, 2), 42)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2],\n",
       "        [ 3,  4],\n",
       "        [ 5,  6],\n",
       "        [42, 42],\n",
       "        [42, 42],\n",
       "        [42, 42]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3 = torch.cat((t1, t2))\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8415,  0.9093],\n",
       "        [ 0.1411, -0.7568],\n",
       "        [-0.9589, -0.2794],\n",
       "        [-0.9165, -0.9165],\n",
       "        [-0.9165, -0.9165],\n",
       "        [-0.9165, -0.9165]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sin(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(12).reshape(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6271, 0.3833, 0.1654, 0.6483],\n",
       "        [0.3568, 0.7267, 0.3471, 0.9081],\n",
       "        [0.7958, 0.8098, 0.4222, 0.9586]], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4 = torch.from_numpy(x)\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "\n",
    "<img src=\"https://i.imgur.com/6Ujttb4.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float32')\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting random weights:\n",
      " tensor([[ 0.8600,  1.2876, -1.8261],\n",
      "        [-1.2874, -0.3288, -0.1104]], requires_grad=True)\n",
      "\n",
      "Starting random bias:\n",
      " tensor([0.2218, 0.4882], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights and biases\n",
    "w = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(2, requires_grad=True)\n",
    "print(f\"Starting random weights:\\n {w}\\n\")\n",
    "print(f\"Starting random bias:\\n {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is simply a function that performs a matrix multiplication of the inputs and the weights w (transposed) and adds the bias b (replicated for each observation).\n",
    "\n",
    "<img src=\"https://i.imgur.com/WGXLFvA.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, w, b):\n",
    "    return x @ w.t() + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  70.7507, -120.2634],\n",
      "        [  74.9226, -152.6581],\n",
      "        [ 141.6676, -161.9712],\n",
      "        [  75.7466, -149.0433],\n",
      "        [  55.3458, -127.6289]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs, w, b)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results represent the number of Apples (col 1) and Oranges (col 2) for each region (row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-14.7507, 190.2634],\n",
      "        [  6.0774, 253.6581],\n",
      "        [-22.6676, 294.9713],\n",
      "        [-53.7466, 186.0433],\n",
      "        [ 47.6542, 246.6289]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compare with targets\n",
    "print(targets-preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate how good/bad the model is doing we introduce a *loss* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss - average of squared residuals\n",
    "def mse(t1, t2):\n",
    "    diff = t1 - t2\n",
    "    return torch.sum(diff * diff) / diff.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28891.6523, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute loss\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In average each prediction differs from the actual value for a number of units equal to the square root of this *loss*. Our goal is to minimize the loss function uswith gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8600,  1.2876, -1.8261],\n",
      "        [-1.2874, -0.3288, -0.1104]], requires_grad=True)\n",
      "tensor([[   937.9697,    245.4474,     42.5748],\n",
      "        [-19725.6875, -21254.3906, -13134.2812]])\n"
     ]
    }
   ],
   "source": [
    "# Gradients for weights\n",
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decrease the loss we must make a small step in the direction opposite of the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "with torch.no_grad():\n",
    "    w -= w.grad * lr\n",
    "    b -= b.grad * lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use torch.no_grad to indicate to PyTorch that we shouldn't track, calculate, or modify gradients while updating the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(19656.0898, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Let's verify that the loss is actually lower\n",
    "loss = mse(model(inputs, w, b), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed, we reset the gradients to zero by invoking the .zero_() method. We need to do this because PyTorch accumulates gradients. Otherwise, the next time we invoke .backward on the loss, the new gradient values are added to the existing gradients, which may lead to unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100 epochs\n",
    "def train_model(w, b, k):\n",
    "    loss_list = []\n",
    "    lr = 1e-5\n",
    "    for i in range(100):\n",
    "        preds = model(inputs, w, b)\n",
    "        loss = mse(preds, targets)\n",
    "        loss_list.append(loss)\n",
    "        print(f\"epoch[{i}]: loss1 = {loss:.2f}\")\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w -= w.grad * lr\n",
    "            b -= b.grad * lr\n",
    "            if( i%k == 0):\n",
    "                w.grad.zero_()\n",
    "                b.grad.zero_()\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[0]: loss1 = 124.91\n",
      "epoch[1]: loss1 = 122.53\n",
      "epoch[2]: loss1 = 122.30\n",
      "epoch[3]: loss1 = 121.84\n",
      "epoch[4]: loss1 = 121.16\n",
      "epoch[5]: loss1 = 120.26\n",
      "epoch[6]: loss1 = 119.13\n",
      "epoch[7]: loss1 = 117.79\n",
      "epoch[8]: loss1 = 116.24\n",
      "epoch[9]: loss1 = 114.48\n",
      "epoch[10]: loss1 = 112.53\n",
      "epoch[11]: loss1 = 110.39\n",
      "epoch[12]: loss1 = 110.18\n",
      "epoch[13]: loss1 = 109.77\n",
      "epoch[14]: loss1 = 109.16\n",
      "epoch[15]: loss1 = 108.34\n",
      "epoch[16]: loss1 = 107.33\n",
      "epoch[17]: loss1 = 106.13\n",
      "epoch[18]: loss1 = 104.73\n",
      "epoch[19]: loss1 = 103.15\n",
      "epoch[20]: loss1 = 101.40\n",
      "epoch[21]: loss1 = 99.47\n",
      "epoch[22]: loss1 = 99.28\n",
      "epoch[23]: loss1 = 98.92\n",
      "epoch[24]: loss1 = 98.36\n",
      "epoch[25]: loss1 = 97.63\n",
      "epoch[26]: loss1 = 96.72\n",
      "epoch[27]: loss1 = 95.64\n",
      "epoch[28]: loss1 = 94.38\n",
      "epoch[29]: loss1 = 92.96\n",
      "epoch[30]: loss1 = 91.38\n",
      "epoch[31]: loss1 = 89.64\n",
      "epoch[32]: loss1 = 89.48\n",
      "epoch[33]: loss1 = 89.15\n",
      "epoch[34]: loss1 = 88.65\n",
      "epoch[35]: loss1 = 87.99\n",
      "epoch[36]: loss1 = 87.17\n",
      "epoch[37]: loss1 = 86.19\n",
      "epoch[38]: loss1 = 85.06\n",
      "epoch[39]: loss1 = 83.78\n",
      "epoch[40]: loss1 = 82.36\n",
      "epoch[41]: loss1 = 80.80\n",
      "epoch[42]: loss1 = 80.65\n",
      "epoch[43]: loss1 = 80.35\n",
      "epoch[44]: loss1 = 79.90\n",
      "epoch[45]: loss1 = 79.31\n",
      "epoch[46]: loss1 = 78.57\n",
      "epoch[47]: loss1 = 77.69\n",
      "epoch[48]: loss1 = 76.67\n",
      "epoch[49]: loss1 = 75.52\n",
      "epoch[50]: loss1 = 74.24\n",
      "epoch[51]: loss1 = 72.83\n",
      "epoch[52]: loss1 = 72.70\n",
      "epoch[53]: loss1 = 72.43\n",
      "epoch[54]: loss1 = 72.02\n",
      "epoch[55]: loss1 = 71.49\n",
      "epoch[56]: loss1 = 70.82\n",
      "epoch[57]: loss1 = 70.03\n",
      "epoch[58]: loss1 = 69.12\n",
      "epoch[59]: loss1 = 68.08\n",
      "epoch[60]: loss1 = 66.92\n",
      "epoch[61]: loss1 = 65.66\n",
      "epoch[62]: loss1 = 65.53\n",
      "epoch[63]: loss1 = 65.29\n",
      "epoch[64]: loss1 = 64.93\n",
      "epoch[65]: loss1 = 64.45\n",
      "epoch[66]: loss1 = 63.85\n",
      "epoch[67]: loss1 = 63.14\n",
      "epoch[68]: loss1 = 62.31\n",
      "epoch[69]: loss1 = 61.38\n",
      "epoch[70]: loss1 = 60.33\n",
      "epoch[71]: loss1 = 59.19\n",
      "epoch[72]: loss1 = 59.08\n",
      "epoch[73]: loss1 = 58.86\n",
      "epoch[74]: loss1 = 58.54\n",
      "epoch[75]: loss1 = 58.10\n",
      "epoch[76]: loss1 = 57.57\n",
      "epoch[77]: loss1 = 56.92\n",
      "epoch[78]: loss1 = 56.18\n",
      "epoch[79]: loss1 = 55.34\n",
      "epoch[80]: loss1 = 54.40\n",
      "epoch[81]: loss1 = 53.37\n",
      "epoch[82]: loss1 = 53.27\n",
      "epoch[83]: loss1 = 53.08\n",
      "epoch[84]: loss1 = 52.78\n",
      "epoch[85]: loss1 = 52.39\n",
      "epoch[86]: loss1 = 51.91\n",
      "epoch[87]: loss1 = 51.33\n",
      "epoch[88]: loss1 = 50.66\n",
      "epoch[89]: loss1 = 49.90\n",
      "epoch[90]: loss1 = 49.05\n",
      "epoch[91]: loss1 = 48.13\n",
      "epoch[92]: loss1 = 48.04\n",
      "epoch[93]: loss1 = 47.86\n",
      "epoch[94]: loss1 = 47.60\n",
      "epoch[95]: loss1 = 47.24\n",
      "epoch[96]: loss1 = 46.81\n",
      "epoch[97]: loss1 = 46.29\n",
      "epoch[98]: loss1 = 45.68\n",
      "epoch[99]: loss1 = 45.00\n"
     ]
    }
   ],
   "source": [
    "losses1 = train_model(w, b,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = torch.randn(2, 3, requires_grad=True)\n",
    "b2 = torch.randn(2, requires_grad=True)\n",
    "losses2 = train_model(w2, b2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-40-a0d2faabd9e9>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-41-c60ea544fe1c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlosses1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"1\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlosses2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"2\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlegend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(range(100), losses1, label=\"1\")\n",
    "plt.plot(range(100), losses2, label=\"2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression using PyTorch built-ins\n",
    "We've implemented linear regression & gradient descent model using some basic tensor operations. However, since this is a common pattern in deep learning, PyTorch provides several built-in functions and classes to make it easy to create and train models with just a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.],\n",
       "        [ 74.,  66.,  43.],\n",
       "        [ 91.,  87.,  65.],\n",
       "        [ 88., 134.,  59.],\n",
       "        [101.,  44.,  37.],\n",
       "        [ 68.,  96.,  71.],\n",
       "        [ 73.,  66.,  44.],\n",
       "        [ 92.,  87.,  64.],\n",
       "        [ 87., 135.,  57.],\n",
       "        [103.,  43.,  36.],\n",
       "        [ 68.,  97.,  70.]])"
      ]
     },
     "execution_count": 917,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], \n",
    "                   [91, 88, 64], \n",
    "                   [87, 134, 58], \n",
    "                   [102, 43, 37], \n",
    "                   [69, 96, 70], \n",
    "                   [74, 66, 43], \n",
    "                   [91, 87, 65], \n",
    "                   [88, 134, 59], \n",
    "                   [101, 44, 37], \n",
    "                   [68, 96, 71], \n",
    "                   [73, 66, 44], \n",
    "                   [92, 87, 64], \n",
    "                   [87, 135, 57], \n",
    "                   [103, 43, 36], \n",
    "                   [68, 97, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119],\n",
    "                    [57, 69], \n",
    "                    [80, 102], \n",
    "                    [118, 132], \n",
    "                    [21, 38], \n",
    "                    [104, 118], \n",
    "                    [57, 69], \n",
    "                    [82, 100], \n",
    "                    [118, 134], \n",
    "                    [20, 38], \n",
    "                    [102, 120]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 919,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define dataset\n",
    "train_ds = TensorDataset(inputs, targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TensorDataset allows us to access a small section of the training data using the array indexing notation ([0:3] in the above code). It returns a tuple with two elements. The first element contains the input variables for the selected rows, and the second contains the targets.\n",
    "\n",
    "We'll also create a DataLoader, which can split the data into batches of a predefined size while training. It also provides other utilities like shuffling and random sampling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Define data loader\n",
    "batch_size = 5\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[103.,  43.,  36.],\n",
      "        [ 68.,  97.,  70.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 88., 134.,  59.],\n",
      "        [101.,  44.,  37.]])\n",
      "tensor([[ 20.,  38.],\n",
      "        [102., 120.],\n",
      "        [ 22.,  37.],\n",
      "        [118., 132.],\n",
      "        [ 21.,  38.]])\n"
     ]
    }
   ],
   "source": [
    "for xb, yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model loading\n",
    "Instead of initializing the weights & biases manually, we can define the model using the nn.Linear class from PyTorch, which does it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3117,  0.1121,  0.0023],\n",
      "        [-0.1589, -0.5105,  0.1858]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3957, 0.3833], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define model\n",
    "model = nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch models also have a helpful .parameters method, which returns a list containing all the weights and bias matrices present in the model. For our linear regression model, we have one weight matrix and one bias matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3117,  0.1121,  0.0023],\n",
       "         [-0.1589, -0.5105,  0.1858]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.3957, 0.3833], requires_grad=True)]"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-14.7476, -37.4269],\n",
       "        [-17.9550, -47.1046],\n",
       "        [-11.5644, -71.0671],\n",
       "        [-26.4928, -30.8973],\n",
       "        [-10.1859, -46.5784],\n",
       "        [-15.1715, -37.0753],\n",
       "        [-18.0648, -46.4083],\n",
       "        [-11.8738, -71.0401],\n",
       "        [-26.0690, -31.2489],\n",
       "        [ -9.8718, -46.2337],\n",
       "        [-14.8574, -36.7306],\n",
       "        [-18.3788, -46.7530],\n",
       "        [-11.4546, -71.7634],\n",
       "        [-26.8069, -31.2420],\n",
       "        [ -9.7620, -46.9300]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15389.6973, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Import nn.functional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "loss = loss_fn(model(inputs), targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that model.parameters() is passed as an argument to optim.SGD so that the optimizer knows which matrices should be modified during the update step. In complex models different branches of the structure can use different optimizers, so not all the parameters of the network must be passed.\n",
    "Also, we can specify a learning rate that controls the amount by which the parameters are modified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt, train_dl):\n",
    "    \n",
    "    # Repeat for given number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Train with batches of data\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            # 1. Generate predictions\n",
    "            pred = model(xb)\n",
    "            \n",
    "            # 2. Calculate loss\n",
    "            loss = loss_fn(pred, yb)\n",
    "            \n",
    "            # 3. Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Update parameters using gradients\n",
    "            opt.step()\n",
    "            \n",
    "            # 5. Reset the gradients to zero\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 358.9918\n",
      "Epoch [20/100], Loss: 512.6168\n",
      "Epoch [30/100], Loss: 55.0282\n",
      "Epoch [40/100], Loss: 100.0373\n",
      "Epoch [50/100], Loss: 119.1583\n",
      "Epoch [60/100], Loss: 5.1483\n",
      "Epoch [70/100], Loss: 25.4652\n",
      "Epoch [80/100], Loss: 40.4350\n",
      "Epoch [90/100], Loss: 19.7466\n",
      "Epoch [100/100], Loss: 15.7992\n"
     ]
    }
   ],
   "source": [
    "fit(100, model, loss_fn, opt, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}